{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e6ac2-06bb-4dc8-983e-404fa217b468",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c816d52663e2b7e",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from io import BytesIO, StringIO\n",
    "import random\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "f = StringIO()\n",
    "with redirect_stdout(f):\n",
    "    help(pow)\n",
    "s = f.getvalue()\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchdata.stateful_dataloader import StatefulDataLoader\n",
    "from vllm import LLM, SamplingParams\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from verl.utils.dataset import RLHFDataset, collate_fn\n",
    "from verl.trainer.config import DataConfig, PPOConfig\n",
    "from verl.utils.tokenizer import get_tokenizer, get_processor\n",
    "from verl.tooluse.parse import Parser\n",
    "from verl.tooluse.tools import *\n",
    "from verl.tooluse.chart_data import *\n",
    "from examples.reward_function.refocus import compute_score\n",
    "\n",
    "# avoid tokenizer parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# set model to eval here !\n",
    "model_name = \"VTOOL/VTOOL-R1-3B-S-V1\"\n",
    "\n",
    "# set paths\n",
    "if not os.path.exists('./results'):\n",
    "    os.mkdir('./results')\n",
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_path = f'./results/3B_{timestamp}.jsonl'\n",
    "stats_path = f'./results/3B_{timestamp}_stats.jsonl'\n",
    "score_path = f'./results/3B_{timestamp}_scores.jsonl'\n",
    "output_img_dir = f'./results/3B_{timestamp}_edited_imgs/'\n",
    "os.mkdir(output_img_dir)\n",
    "\n",
    "\n",
    "llm = LLM(model_name, limit_mm_per_prompt={\"image\": 2})\n",
    "processor = get_processor(model_name)\n",
    "tokenizer = get_tokenizer(model_name)\n",
    "sampling_params = SamplingParams(temperature=0.5, top_p=0.99, max_tokens=1024)  # TODO: not used\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f36d58d0fffb722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_path = \"../examples/tooluse_config.yaml\"\n",
    "\n",
    "config = OmegaConf.load(config_path).data\n",
    "format_prompt_path = \"../examples/format_prompt/chartQA.jinja\"\n",
    "config.val_batch_size = 5\n",
    "\n",
    "val_dataset = RLHFDataset(\n",
    "    data_path='../val_full.parquet',\n",
    "    tokenizer=tokenizer,\n",
    "    processor=processor,\n",
    "    prompt_key=config.prompt_key,\n",
    "    answer_key=config.answer_key,\n",
    "    image_key=config.image_key,\n",
    "    max_prompt_length=config.max_prompt_length,\n",
    "    truncation=\"right\",\n",
    "    format_prompt=format_prompt_path,\n",
    "    min_pixels=config.min_pixels,\n",
    "    max_pixels=config.max_pixels,\n",
    "    filter_overlong_prompts=config.filter_overlong_prompts,\n",
    ")\n",
    "\n",
    "val_dataloader = StatefulDataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=len(val_dataset) if config.val_batch_size == -1 else config.val_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "print(len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0e55a1-be7d-4548-84f7-05cffb85e0dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for entry in val_dataloader:\n",
    "    print(entry.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58442ca4019f806b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tool_parser = Parser()\n",
    "\"\"\"\n",
    "Variables\n",
    "\"\"\"\n",
    "# capture code exec\n",
    "\n",
    "\n",
    "def display(obj):\n",
    "    global captured_output\n",
    "    captured_output = obj\n",
    "    \n",
    "def get_tool_context():\n",
    "    context = {\n",
    "        \"display\": display,\n",
    "        \"focus_on_columns_with_mask\": focus_on_columns_with_mask,\n",
    "        \"focus_on_rows_with_mask\": focus_on_rows_with_mask,\n",
    "        \"focus_on_columns_with_draw\": focus_on_columns_with_draw,\n",
    "        \"focus_on_rows_with_draw\": focus_on_rows_with_draw,\n",
    "        \"focus_on_columns_with_highlight\": focus_on_columns_with_highlight,\n",
    "        \"focus_on_rows_with_highlight\": focus_on_rows_with_highlight,\n",
    "        \"focus_on_x_values_with_mask\": focus_on_x_values_with_mask,\n",
    "        \"focus_on_y_values_with_mask\": focus_on_y_values_with_mask,\n",
    "        \"focus_on_x_values_with_draw\": focus_on_x_values_with_draw,\n",
    "        \"focus_on_y_values_with_draw\": focus_on_y_values_with_draw,\n",
    "        \"focus_on_x_values_with_highlight\": focus_on_x_values_with_highlight,\n",
    "        \"focus_on_y_values_with_highlight\": focus_on_y_values_with_highlight,\n",
    "    }\n",
    "    return context\n",
    "\n",
    "# stats\n",
    "num_tool_calls = 0\n",
    "num_direct = 0\n",
    "num_success_tool_calls = 0\n",
    "num_failed_tool_calls = 0\n",
    "\n",
    "\"\"\"\n",
    "Eval Loop\n",
    "\"\"\"\n",
    "merged_outputs_eval = []\n",
    "correct_or_not_eval = []\n",
    "result_objs = []\n",
    "\n",
    "for entry in tqdm(val_dataloader):\n",
    "    # print(entry.keys())\n",
    "    # print(entry['raw_prompt_ids'])\n",
    "    # print(entry['multi_modal_data'])\n",
    "    # print(entry['input_ids'])\n",
    "    # print(entry['multi_modal_data'])\n",
    "    \n",
    "#     input_ids: torch.Tensor = entry['input_ids'] # (bs, prompt_length)\n",
    "#     attention_mask: torch.Tensor = entry[\"attention_mask\"]\n",
    "#     position_ids: torch.Tensor = entry[\"position_ids\"]\n",
    "#     # raw_prompt_ids: torch.Tensor = entry[\"raw_prompt_ids\"]\n",
    "#     batch_size = input_ids.size(0)\n",
    "    metadata_batch = entry['metadata']\n",
    "    prompts = entry['prompt']\n",
    "    figure_paths = entry['figure_path']\n",
    "    \n",
    "    ### FIRST ROLLOUT\n",
    "    print('=== starting first rollout ===')\n",
    "    vllm_inputs = [\n",
    "        {\"prompt_token_ids\": list(ids), \"multi_modal_data\": data} \n",
    "        for ids, data in zip(entry[\"raw_prompt_ids\"], entry[\"multi_modal_data\"])\n",
    "    ]\n",
    "    \n",
    "    outputs = llm.generate(\n",
    "        prompts=vllm_inputs,\n",
    "        sampling_params=sampling_params,\n",
    "    )\n",
    "    print('--- first rollout response ---')\n",
    "    print(outputs[0].outputs[0].text)\n",
    "    print('--- END first rollout response ---')\n",
    "    \n",
    "    \n",
    "    parsed_results = [tool_parser.parse(output.outputs[0].text) for output in outputs]\n",
    "    \n",
    "    edited_images = [] # store edited image for second rollout\n",
    "    code_exec_errors = [] # store code exec error\n",
    "    code_exec_stdouts = [] # store code exec stdout\n",
    "    tool_use_indices = [] # store which entry had successful code exec\n",
    "    second_rollout_datas = [] # store model input data for second rollout\n",
    "    parsed_codes = []\n",
    "        \n",
    "    # this actually takes some time\n",
    "    for idx, result in enumerate(parsed_results):\n",
    "        if not result[\"status\"]:\n",
    "\n",
    "            if result[\"error_code\"] == \"NOTOOL\":\n",
    "                num_direct += 1\n",
    "            else:\n",
    "                num_tool_calls += 1\n",
    "                num_failed_tool_calls += 1\n",
    "                \n",
    "            edited_images.append(None)\n",
    "            code_exec_errors.append(None)\n",
    "            code_exec_stdouts.append(None)\n",
    "            parsed_codes.append(None)\n",
    "\n",
    "            continue\n",
    "\n",
    "        num_tool_calls += 1\n",
    "\n",
    "        metadata = metadata_batch[idx]\n",
    "\n",
    "        # ### keep these for 'partial' data\n",
    "        # y_values = metadata[\"y_values\"]\n",
    "        # y_bboxes = metadata[\"y_bboxes\"]\n",
    "        # headers = y_values  # these are your column names\n",
    "        # bbox_mapping = {label: bbox for label, bbox in zip(y_values, y_bboxes)}\n",
    "        # ### end keep\n",
    "\n",
    "        ### keep these for 'full' data\n",
    "        if metadata[\"type\"] == \"v_bar\":\n",
    "            bbox_mapping = metadata[\"x_bbox\"]\n",
    "        elif metadata[\"type\"] == \"h_bar\":\n",
    "            bbox_mapping = metadata[\"y_values_bbox\"]\n",
    "        ### end keep\n",
    "\n",
    "        ### execute code\n",
    "        #code_executor = CodeExecutor(\"executor\")\n",
    "        code = result[\"content\"]\n",
    "        #exit_code, output, file_paths = code_executor.execute(result[\"content\"])\n",
    "        figure_path = figure_paths[idx]\n",
    "\n",
    "        print('--- parsed code ---')\n",
    "        print(code)\n",
    "        print('--- END parsed code ---')\n",
    "        successful = True\n",
    "        \n",
    "        captured_output = None\n",
    "\n",
    "        context = get_tool_context()\n",
    "\n",
    "        context[\"image_1\"] = Image.open('../'+figure_path)\n",
    "        context[\"columns_bbox\"] = bbox_mapping\n",
    "        context[\"rows_bbox\"] = bbox_mapping\n",
    "\n",
    "        code_exec_error = None\n",
    "        code_exec_stdout = None\n",
    "        try:\n",
    "            f = StringIO()\n",
    "            with redirect_stdout(f):\n",
    "                exec(code, context)\n",
    "        except BaseException as e:\n",
    "            successful = False\n",
    "            print('~~~ code error ~~~')\n",
    "            print(f\"{e}\")\n",
    "            print('~~~ END code error ~~~')\n",
    "            code_exec_error = f\"{e}\"\n",
    "        code_exec_stdout = f.getvalue()\n",
    "        \n",
    "        # log code exec\n",
    "        code_exec_errors.append(code_exec_error)\n",
    "        code_exec_stdouts.append(code_exec_stdout)\n",
    "        parsed_codes.append(code)\n",
    "\n",
    "        # if successful code exec\n",
    "        if successful:\n",
    "            if captured_output is not None:\n",
    "                successful = isinstance(captured_output, Image.Image)\n",
    "                print('~~~ captured output ~~~')\n",
    "                print(captured_output)\n",
    "                print('~~~ END captured output ~~~')\n",
    "                '''try:\n",
    "                    #self.captured_output.save(f\"tmp_vis/{figure_ids[idx]}.png\")\n",
    "                    with open(f\"tmp_vis/{figure_ids[idx]}.txt\", 'w') as file:\n",
    "                        file.write(output_texts[idx])\n",
    "                    #the AI may somehow give dicts which is wrong!!!!!\n",
    "                except Exception as e:\n",
    "                    successful = False'''\n",
    "            else:\n",
    "                successful = False\n",
    "\n",
    "        if successful:\n",
    "            num_success_tool_calls += 1\n",
    "\n",
    "            edited_images.append(captured_output)\n",
    "\n",
    "            trim_to_action_end = tool_parser.trim_to_action_end(outputs[idx].outputs[0].text)\n",
    "\n",
    "            #we need to add image repsonse here:\n",
    "            trim_to_action_end += \"\\nOBSERVATION: Execution success. The output is as follows:\"\n",
    "            trim_to_action_end += \"\\n<the image outputs of the code is added as the second image>\"\n",
    "\n",
    "            #image isn't actually used here so we insert two dummy images\n",
    "            messages = val_dataloader.dataset.tu_build_message(prompts[idx], [None, None], trim_to_action_end)\n",
    "\n",
    "            prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "            original_image = Image.open('../' + figure_path)\n",
    "            edited_image = captured_output\n",
    "\n",
    "            images = [val_dataloader.dataset.process_image(image) for image in [original_image, edited_image]]\n",
    "\n",
    "            model_inputs = processor(images, [prompt], add_special_tokens=False, return_tensors=\"pt\")\n",
    "            input_ids = model_inputs.pop(\"input_ids\")[0]\n",
    "            attention_mask = model_inputs.pop(\"attention_mask\")[0]\n",
    "\n",
    "            #we assume this is not for QWEN2, see dataset.py for code\n",
    "            position_ids = torch.clip(attention_mask.cumsum(dim=0) - 1, min=0, max=None)  # (seq_length,)\n",
    "\n",
    "            second_rollout_data = {}\n",
    "\n",
    "            second_rollout_data[\"multi_modal_data\"] = {\"image\": images}\n",
    "            second_rollout_data[\"multi_modal_inputs\"] = dict(model_inputs)\n",
    "\n",
    "            max_prompt_length = val_dataloader.dataset.max_prompt_length\n",
    "\n",
    "            truncation = val_dataloader.dataset.truncation\n",
    "            raw_prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "            if len(raw_prompt_ids) > max_prompt_length:\n",
    "                if truncation == \"left\":\n",
    "                    raw_prompt_ids = raw_prompt_ids[-max_prompt_length :]\n",
    "                elif truncation == \"right\":\n",
    "                    raw_prompt_ids = raw_prompt_ids[: max_prompt_length]\n",
    "                elif truncation == \"error\":\n",
    "                    raise RuntimeError(f\"Prompt length {len(raw_prompt_ids)} is longer than {max_prompt_length}.\")\n",
    "            \n",
    "            second_rollout_data[\"raw_prompt_ids\"] = raw_prompt_ids\n",
    "            second_rollout_data[\"metadata\"] = metadata_batch[idx]\n",
    "\n",
    "            second_rollout_datas.append(second_rollout_data)\n",
    "            tool_use_indices.append(idx)\n",
    "\n",
    "        else:\n",
    "            num_failed_tool_calls += 1\n",
    "            edited_images.append(None)\n",
    "\n",
    "        print(f\"SUCCESSFUL? {successful}\")\n",
    "        print(\"------\")\n",
    "            \n",
    "    second_rollout_batch_dict = collate_fn(second_rollout_datas)\n",
    "    \n",
    "    print(f'failed tool call count: {num_failed_tool_calls}')\n",
    "    print('ready for second rollout')\n",
    "    # print(second_rollout_batch_dict)\n",
    "            \n",
    "    ### SECOND ROLLOUT\n",
    "    if second_rollout_batch_dict == {}:\n",
    "        print('=== nothing to do in second rollout ===')\n",
    "    else:\n",
    "        print('=== starting second rollout ===')\n",
    "        second_vllm_inputs = [\n",
    "            {\"prompt_token_ids\": list(ids), \"multi_modal_data\": data} \n",
    "            for ids, data in zip(second_rollout_batch_dict[\"raw_prompt_ids\"], second_rollout_batch_dict[\"multi_modal_data\"])\n",
    "        ]\n",
    "\n",
    "        second_outputs = llm.generate(\n",
    "            prompts=second_vllm_inputs,\n",
    "            sampling_params=sampling_params,\n",
    "        )\n",
    "        print('--- second rollout response ---')\n",
    "        print(second_outputs[0].outputs[0].text)\n",
    "        print('--- END second rollout response ---')\n",
    "    \n",
    "    ### collate results \n",
    "    merged_outputs = []\n",
    "    second_cnt = 0\n",
    "    for idx, edited_image in enumerate(edited_images):\n",
    "        if edited_image == None:  # code exec failed or NO TOOL\n",
    "            merged_outputs.append(outputs[idx])\n",
    "        else:\n",
    "            merged_outputs.append(second_outputs[second_cnt])\n",
    "            second_cnt += 1\n",
    "    \n",
    "    ### batch eval result\n",
    "    correct_or_not = []\n",
    "    print(entry['ground_truth'])\n",
    "    for idx, gt in enumerate(entry['ground_truth']):\n",
    "        model_response_text = merged_outputs[idx].outputs[0].text\n",
    "        model_response_choice = re.findall(r'FINAL ANSWER:\\s*(.*?)(?=\\.\\s|\\.?$)', model_response_text)[-1]\n",
    "        if model_response_choice.lower() == gt.lower():\n",
    "            print(f'correct: {gt}')\n",
    "            correct_or_not.append(True)\n",
    "        else:\n",
    "            print(f'wrong: {model_response_choice} | gt: {gt}')\n",
    "            correct_or_not.append(False)\n",
    "    \n",
    "    ### log results\n",
    "    with open(output_path, \"a\") as out_f:\n",
    "        for idx in range(len(outputs)):\n",
    "            # write edited img to path\n",
    "            edited_img_path = None\n",
    "            if edited_images[idx]:\n",
    "                edited_img_path = output_img_dir + f\"/{entry['figure_id'][idx]}_edited.png\"\n",
    "                edited_images[idx].save(edited_img_path)\n",
    "            \n",
    "            result_obj = {\n",
    "                'first_rollout_response': outputs[idx].outputs[0].text,\n",
    "                'second_rollout_response': merged_outputs[idx].outputs[0].text if edited_images[idx] else None,\n",
    "                'model_response': merged_outputs[idx].outputs[0].text,\n",
    "                'code': parsed_codes[idx],\n",
    "                'code_error': code_exec_errors[idx],\n",
    "                'code_stdout': code_exec_stdouts[idx],\n",
    "                'original_figure_path': entry['figure_path'][idx],\n",
    "                'edited_figure_path': edited_img_path,\n",
    "                'ground_truth': entry['ground_truth'][idx],\n",
    "                'query': entry['query'][idx]\n",
    "                'prompt': entry['prompt'][idx],\n",
    "            }\n",
    "            result_objs.append(result_obj)\n",
    "            out_f.write(json.dumps(result_obj)+'\\n')\n",
    "            \n",
    "    merged_outputs_eval += merged_outputs\n",
    "    correct_or_not_eval += correct_or_not\n",
    "    \n",
    "    \n",
    "    print('end batch')\n",
    "    print(f'logged {len(result_objs)} results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554fc9de09480c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "score\n",
    "\"\"\"\n",
    "\n",
    "gpt_scores = []\n",
    "\n",
    "for result in tqdm(result_objs):\n",
    "    gpt_score, prediction = compute_acc_from_raw_answer(result['query'], result['ground_truth'], result['model_response'])\n",
    "    with open(score_path, 'a') as f:\n",
    "        f.write(str(gpt_score))\n",
    "    gpt_scores.append(gpt_score)\n",
    "    \n",
    "print('GPT scored acc: ', len(filter(lambda x: x == 1, gpt_scores)) / len(gpt_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbc28bd-e509-4a64-ad69-27eb2f039334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print stats\n",
    "\"\"\"\n",
    "stats_obj = {\n",
    "    'gpt_scored_acc' = len(filter(lambda x: x == 1, gpt_scores)) / len(gpt_scores),\n",
    "    'exact_match_acc' = sum(correct_or_not_eval) / len(correct_or_not_eval),\n",
    "    'num_tool_calls' = num_tool_calls,\n",
    "    'num_direct' = num_direct,\n",
    "    'num_success_tool_calls' = num_success_tool_calls,\n",
    "    'num_failed_tool_calls' = num_failed_tool_calls\n",
    "}\n",
    "\n",
    "print(stats_obj)\n",
    "\n",
    "with open(stats_path, 'r') as f:\n",
    "    f.write(json.dumps(stats_obj))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
